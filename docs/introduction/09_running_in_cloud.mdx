---
id: running-in-cloud
title: Running in cloud
---

import MainExample from '!!raw-loader!./code/09_apify_sdk.py';

## Apify Platform

Crawlee is developed by [**Apify**](https://apify.com), the web scraping and automation platform. You could say it is the **home of Crawlee projects**. In this section you'll see how to deploy the crawler there with just a few simple steps. You can deploy a **Crawlee** project wherever you want, but using the [**Apify Platform**](https://console.apify.com) will give you the best experience.

<!-- In case you want to deploy your Crawlee project to other platforms, check out the [**Deployment**](../deployment) section. -->

With a few simple steps, you can convert your Crawlee project into a so-called **Actor**. Actors are serverless micro-apps that are easy to develop, run, share, and integrate. The infra, proxies, and storages are ready to go. [Learn more about Actors](https://apify.com/actors).

<!-- :::info Choosing between Crawlee CLI and Apify CLI for project setup -->
<!---->
<!-- We started this guide by using the Crawlee CLI to bootstrap the project - it offers the basic Crawlee templates, including a ready-made `Dockerfile`. If you know you will be deploying your project to the Apify Platform, you might want to start with the Apify CLI instead. It also offers several project templates, and those are all set up to be used on the Apify Platform right ahead. -->
<!---->
<!-- ::: -->

## Dependencies

The first step will be installing two new dependencies:

- Apify SDK, a toolkit for working with the Apify Platform. This will allow us to wire the storages (e.g. `RequestQueue` and `Dataset`) to the Apify cloud products. This will be a dependency of our project.

    ```bash
    poetry add apify
    ```

- Alternatively, if you don't use `poetry` to manage your project, you may just install the SDK with `pip`:

    ```bash
    pip install apify
    ```


- Apify CLI, a command-line tool that will help us with authentication and deployment. This will be a globally installed tool, you will install it only once and use it in all your Crawlee/Apify projects.

    ```bash
    npm install -g apify-cli
    ```

## Logging in to the Apify Platform

The next step will be [creating your Apify account](https://console.apify.com/sign-up). Don't worry, we have a **free tier**, so you can try things out before you buy in! Once you have that, it's time to log in with the just-installed [Apify CLI](https://docs.apify.com/cli/). You will need your personal access token, which you can find at https://console.apify.com/account#/integrations.

```bash
apify login
```

## Adjusting the code

Now that you have your account set up, you will need to adjust the code a tiny bit. We will use the [Apify SDK](https://docs.apify.com/sdk/js/), which will help us to wire the Crawlee storages (like the `RequestQueue`) to their Apify Platform counterparts - otherwise Crawlee would keep things only in memory.

Open your `src/main.py` file, and wrap everyting in your `main` function with the `Actor` context manager. Your code should look like this:

<CodeBlock className="language-python" title="src/main.py">
    {MainExample}
</CodeBlock>

The context manager will configure Crawlee to use the Apify API instead of its default memory storage interface. It also sets up few other things, like listening to the platform events via websockets. After the body is finished, it handles graceful shutdown.

:::info Understanding `async with Actor` behavior with environment variables

The `Actor` context manager works conditionally based on the environment variables, namely based on the `APIFY_IS_AT_HOME` env var, which is set to `true` on the Apify Platform. This means that your project will remain working the same locally, but will use the Apify API when deployed to the Apify Platform.

:::

## Initializing the project

You will also need to initialize the project for Apify, to do that, use the Apify CLI again:

```bash
apify init
```

This will create a folder called `.actor`, and an `actor.json` file inside it - this file contains the configuration relevant to the Apify Platform, namely the Actor name, version, build tag, and few other things. Check out the [relevant documentation](https://docs.apify.com/platform/actors/development/actor-definition/actor-json) to see all the different things you can set there up.

## Ship it!

And that's all, your project is now ready to be published on the Apify Platform. You can use the Apify CLI once more to do that:

```bash
apify push
```

This command will create an archive from your project, upload it to the Apify Platform and initiate a Docker build. Once finished, you will get a link to your new Actor on the platform.

## Learning more about web scraping

:::tip Explore Apify Academy Resources

If you want to learn more about web scraping and browser automation, check out the [Apify Academy](https://developers.apify.com/academy). It's full of courses and tutorials on the topic. From beginner to advanced. And the best thing: **It's free and open source** ‚ù§Ô∏è

<!-- If you want to do one more project, checkout our tutorial on building a [HackerNews scraper using Crawlee](https://blog.apify.com/crawlee-web-scraping-tutorial/). -->

:::

## Thank you! üéâ

That's it! Thanks for reading the whole introduction and if there's anything wrong, please üôè let us know on [GitHub](https://github.com/apify/crawlee-python) or in our [Discord community](https://discord.com/invite/jyEM2PRvMU). Happy scraping! üëã
