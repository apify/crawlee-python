---
id: crawler-with-custom-parser
title: Crawler with custom parser
description: Learn how to use HttpCrawler with third-party parsing libraries and how to create a custom crawler with full framework integration.
---

import ApiLink from '@site/src/components/ApiLink';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';

import LxmlParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lxml_parser.py';
import LxmlSaxoncheParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lxml_saxonche_parser.py';
import LexborParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lexbor_parser.py';
import PyqueryParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/pyquery_parser.py';
import ScraplingParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/scrapling_parser.py';

import SelectolaxParserSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_parser.py';
import SelectolaxContextSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_context.py';
import SelectolaxCrawlerSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_crawler.py';
import SelectolaxCrawlerRunSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_crawler_run.py';

Crawlee provides <ApiLink to="class/BeautifulSoupCrawler">`BeautifulSoupCrawler`</ApiLink> and <ApiLink to="class/ParselCrawler">`ParselCrawler`</ApiLink> as built-in solutions for HTML parsing. However, you may want to use a different parsing library that better fits your specific needs.

There are two approaches to integrate a custom parser:

- **Using <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink>** — Parse raw responses directly in request handlers. Quick to set up, but helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink> are not available. Best for simple scraping tasks or quick prototyping.
- **Creating a custom crawler** — Implement a crawler based on <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink> for full framework integration. Best for reusable crawlers or when you need full integration and support in <ApiLink to="class/AdaptivePlaywrightCrawler">`AdaptivePlaywrightCrawler`</ApiLink>.

## Using HttpCrawler with custom parser

The <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink> gives you direct access to raw HTTP responses, allowing you to integrate any parsing library of your choice. When using this approach, helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink> and <ApiLink to="class/ExtractLinksFunction">`extract_links`</ApiLink> are not available, and it requires minimal setup.

The following sections demonstrate how to use various parsing libraries with <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink> to extract data from a page and enqueue discovered links for further crawling.

### lxml

[lxml](https://lxml.de/) is a high-performance XML and HTML parser that provides Python bindings to the C libraries libxml2 and libxslt. It supports XPath 1.0, XSLT 1.0, and EXSLT extensions for element selection. The `make_links_absolute` method is particularly useful for converting relative URLs to absolute ones before link extraction.

<RunnableCodeBlock className="language-python" language="python">
    {LxmlParser}
</RunnableCodeBlock>

### lxml with SaxonC-HE

Using [SaxonC-HE](https://pypi.org/project/saxonche/) together with lxml enables XPath 3.1 support, which provides advanced features like `distinct-values()` function and more powerful string manipulation. In this setup, lxml converts HTML to well-formed XML that SaxonC-HE can process.

<RunnableCodeBlock className="language-python" language="python">
    {LxmlSaxoncheParser}
</RunnableCodeBlock>

### selectolax

[selectolax](https://github.com/rushter/selectolax) is a fast HTML parser that offers two backends: the default `Modest` engine and `Lexbor`. It provides a simple API with CSS selector support. The example below uses the `Lexbor` backend for optimal performance.

<RunnableCodeBlock className="language-python" language="python">
    {LexborParser}
</RunnableCodeBlock>

### PyQuery

[PyQuery](https://pyquery.readthedocs.io/) brings jQuery-like syntax to Python for HTML manipulation. Built on top of `lxml`, it combines familiar jQuery CSS selectors with Python's ease of use. This is a good choice if you're comfortable with jQuery syntax and want a straightforward API for DOM traversal and manipulation.

<RunnableCodeBlock className="language-python" language="python">
    {PyqueryParser}
</RunnableCodeBlock>

### Scrapling

[Scrapling](https://github.com/D4Vinci/Scrapling) is a scraping library that provides both CSS selectors and XPath 1.0. It offers automatic text extraction and a Scrapy/BeautifulSoup-like API with pseudo-elements support similar to Parsel.

<RunnableCodeBlock className="language-python" language="python">
    {ScraplingParser}
</RunnableCodeBlock>

## Creating a custom crawler

For deeper integration with full access to framework helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>, you can create a custom crawler based on <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink>. This approach requires implementing three components — a parser, a crawling context, and the crawler class — but provides a seamless experience similar to built-in crawlers.

The following example demonstrates how to create a custom crawler using `selectolax` with the `Lexbor` engine.

### Implementing the parser

The parser converts HTTP responses into a parsed document and provides methods for element selection. Implement <ApiLink to="class/AbstractHttpParser">`AbstractHttpParser`</ApiLink> using `selectolax` with required methods for parsing and querying:

<CodeBlock className="language-python" language="python" title="selectolax_parser.py">
    {SelectolaxParserSource}
</CodeBlock>

### Defining the crawling context

The crawling context is passed to request handlers and provides access to the parsed content. Extend <ApiLink to="class/ParsedHttpCrawlingContext">`ParsedHttpCrawlingContext`</ApiLink> to define the interface your handlers will work with:

<CodeBlock className="language-python" language="python" title="selectolax_context.py">
    {SelectolaxContextSource}
</CodeBlock>

### Building the crawler

The crawler class connects the parser and context. Extend <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink> and configure the context pipeline to use your custom components:

<CodeBlock className="language-python" language="python" title="selectolax_crawler.py">
    {SelectolaxCrawlerSource}
</CodeBlock>

### Using the crawler

The custom crawler works like any built-in crawler. Request handlers receive your custom context with full access to framework helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>:

<CodeBlock className="language-python" language="python">
    {SelectolaxCrawlerRunSource}
</CodeBlock>

## Conclusion

Crawlee offers flexible options for integrating custom parsing libraries. Use <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink> for quick integration when you need to parse responses with your preferred library. For full framework integration with helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>, implement a custom crawler using <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink>. Both approaches allow you to leverage any parser from the Python ecosystem while benefiting from Crawlee's request management, rate limiting, and data storage features.
