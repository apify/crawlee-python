---
id: crawler-with-custom-parser
title: Crawler with custom parser
description: Learn how to use HttpCrawler with third-party parsing libraries and how to create a custom crawler with full framework integration.
---

import ApiLink from '@site/src/components/ApiLink';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import RunnableCodeBlock from '@site/src/components/RunnableCodeBlock';

import LxmlParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lxml_parser.py';
import LxmlSaxoncheParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lxml_saxonche_parser.py';
import LexborParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/lexbor_parser.py';
import PyqueryParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/pyquery_parser.py';
import ScraplingParser from '!!raw-loader!roa-loader!./code_examples/crawler_custom_parser/scrapling_parser.py';

import SelectolaxParserSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_parser.py';
import SelectolaxContextSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_context.py';
import SelectolaxCrawlerSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_crawler.py';
import SelectolaxCrawlerRunSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_crawler_run.py';
import AdaptiveCrawlerRunSource from '!!raw-loader!./code_examples/crawler_custom_parser/selectolax_adaptive_run.py';

Crawlee provides <ApiLink to="class/BeautifulSoupCrawler">`BeautifulSoupCrawler`</ApiLink> and <ApiLink to="class/ParselCrawler">`ParselCrawler`</ApiLink> as built-in solutions for HTML parsing. However, you may want to use a different parsing library that better fits your specific needs.

There are two approaches to integrate a custom parser:

- **Using <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink>** — Parse raw responses directly in request handlers. Quick to set up, but helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink> are not available. Best for simple scraping tasks or quick prototyping.
- **Creating a custom crawler** — Implement a crawler based on <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink> for full framework integration. Best for reusable crawlers or when you need full integration and support in <ApiLink to="class/AdaptivePlaywrightCrawler">`AdaptivePlaywrightCrawler`</ApiLink>.

## Using HttpCrawler with custom parser

The <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink> gives you direct access to raw HTTP responses, allowing you to integrate any parsing library of your choice. When using this approach, helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink> and <ApiLink to="class/ExtractLinksFunction">`extract_links`</ApiLink> are not available, and it requires minimal setup.

The following examples demonstrate integration with various parsing libraries: [lxml](https://lxml.de/) for high-performance XPath 1.0 parsing, [lxml with SaxonC-HE](https://pypi.org/project/saxonche/) for XPath 3.1 support, [selectolax](https://github.com/rushter/selectolax) for fast CSS selector-based parsing, [PyQuery](https://pyquery.readthedocs.io/) for jQuery-like syntax, and [scrapling](https://github.com/D4Vinci/Scrapling) for CSS and XPath selectors with Scrapy/Parsel-like API and BeautifulSoup-style find methods.

<Tabs groupId="custom_parsers">
    <TabItem value="lxml" label="lxml">
        <RunnableCodeBlock className="language-python" language="python">
            {LxmlParser}
        </RunnableCodeBlock>
    </TabItem>
    <TabItem value="saxonche" label="lxml with SaxonC-HE">
        <RunnableCodeBlock className="language-python" language="python">
            {LxmlSaxoncheParser}
        </RunnableCodeBlock>
    </TabItem>
    <TabItem value="selectolax" label="selectolax">
        <RunnableCodeBlock className="language-python" language="python">
            {LexborParser}
        </RunnableCodeBlock>
    </TabItem>
    <TabItem value="pyquery" label="PyQuery">
        <RunnableCodeBlock className="language-python" language="python">
            {PyqueryParser}
        </RunnableCodeBlock>
    </TabItem>
    <TabItem value="scrapling" label="Scrapling">
        <RunnableCodeBlock className="language-python" language="python">
            {ScraplingParser}
        </RunnableCodeBlock>
    </TabItem>
</Tabs>

## Creating a custom crawler

For deeper integration with full access to framework helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>, you can create a custom crawler based on <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink>. This approach requires implementing three components — a parser, a crawling context, and the crawler class — but provides a seamless experience similar to built-in crawlers.

The following example demonstrates how to create a custom crawler using `selectolax` with the `Lexbor` engine.

### Implementing the parser

The parser converts HTTP responses into a parsed document and provides methods for element selection. Implement <ApiLink to="class/AbstractHttpParser">`AbstractHttpParser`</ApiLink> using `selectolax` with required methods for parsing and querying:

<CodeBlock className="language-python" language="python" title="selectolax_parser.py">
    {SelectolaxParserSource}
</CodeBlock>

### Defining the crawling context

The crawling context is passed to request handlers and provides access to the parsed content. Extend <ApiLink to="class/ParsedHttpCrawlingContext">`ParsedHttpCrawlingContext`</ApiLink> to define the interface your handlers will work with:

<CodeBlock className="language-python" language="python" title="selectolax_context.py">
    {SelectolaxContextSource}
</CodeBlock>

### Building the crawler

The crawler class connects the parser and context. Extend <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink> and configure the context pipeline to use your custom components:

<CodeBlock className="language-python" language="python" title="selectolax_crawler.py">
    {SelectolaxCrawlerSource}
</CodeBlock>

### Using the crawler

The custom crawler works like any built-in crawler. Request handlers receive your custom context with full access to framework helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>. Additionally, the custom parser can be used with <ApiLink to="class/AdaptivePlaywrightCrawler">`AdaptivePlaywrightCrawler`</ApiLink> for adaptive crawling:

<Tabs groupId="crawlers">
    <TabItem value="selectolax_crawler" label="SelectolaxCrawler">
        <CodeBlock className="language-python" language="python">
            {SelectolaxCrawlerRunSource}
        </CodeBlock>
    </TabItem>
    <TabItem value="adaptive_playwright_crawler" label="AdaptivePlaywrightCrawler with SelectolaxParser">
        <CodeBlock className="language-python" language="python">
            {AdaptiveCrawlerRunSource}
        </CodeBlock>
    </TabItem>
</Tabs>


## Conclusion

Crawlee offers flexible options for integrating custom parsing libraries. Use <ApiLink to="class/HttpCrawler">`HttpCrawler`</ApiLink> for quick integration when you need to parse responses with your preferred library. For full framework integration with helpers like <ApiLink to="class/EnqueueLinksFunction">`enqueue_links`</ApiLink>, implement a custom crawler using <ApiLink to="class/AbstractHttpCrawler">`AbstractHttpCrawler`</ApiLink>. Both approaches allow you to leverage any parser from the Python ecosystem while benefiting from Crawlee's request management, rate limiting, and data storage features.
