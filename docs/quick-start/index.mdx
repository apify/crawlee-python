markdown
Copy code

---

id: quick-start
title: Quick start

---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

This short tutorial will help you start scraping with Crawlee in just a minute or two. For an in-depth understanding of how Crawlee works, check out the [Introduction](../introduction/index.mdx) section, which provides a comprehensive step-by-step guide to creating your first scraper.

## Choose your crawler

Crawlee offers two main crawler classes: `BeautifulSoupCrawler`, and `PlaywrightCrawler`. All crawlers share the same interface, providing maximum flexibility when switching between them.

### BeautifulSoupCrawler

The `BeautifulSoupCrawler` is a plain HTTP crawler that parses HTML using the well-known [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) library. It crawls the web using an HTTP client that mimics a browser. This crawler is very fast and efficient but cannot handle JavaScript rendering.

### PlaywrightCrawler

The `PlaywrightCrawler` uses a headless browser controlled by the [Playwright](https://playwright.dev/) library. It can manage Chromium, Firefox, Webkit, and other browsers. Playwright is the successor to the [Puppeteer](https://pptr.dev/) library and is becoming the de facto standard in headless browser automation. If you need a headless browser, choose Playwright.

:::caution Before You Start
Crawlee requires Python 3.9 or later.
:::

## Installation

Crawlee is available as the [`crawlee`](https://pypi.org/project/crawlee/) PyPI package.

```sh
pip install crawlee
Additional, optional dependencies unlocking more features are shipped as package extras.

If you plan to use BeautifulSoupCrawler, install crawlee with beautifulsoup extra:

sh
Copy code
pip install 'crawlee[beautifulsoup]'
If you plan to use PlaywrightCrawler, install crawlee with the playwright extra:

sh
Copy code
pip install 'crawlee[playwright]'
Then, install the Playwright dependencies:

sh
Copy code
playwright install
You can install multiple extras at once by using a comma as a separator:

sh
Copy code
pip install 'crawlee[beautifulsoup,playwright]'
Crawling with Crawlee
To start crawling with Crawlee, you can use either the BeautifulSoupCrawler or the PlaywrightCrawler. Below are examples of how to use each crawler:

<Tabs groupId="main">
<TabItem value="BeautifulSoupCrawler" label="BeautifulSoupCrawler">
python
Copy code
import asyncio
from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext

async def main() -> None:
    # Create an instance of BeautifulSoupCrawler
    crawler = BeautifulSoupCrawler(max_requests_per_crawl=50)

    # Define a request handler to process each crawled page
    @crawler.router.default_handler
    async def request_handler(context: BeautifulSoupCrawlingContext) -> None:
        # Extract relevant data from the page context
        data = {
            'url': context.request.url,
            'title': context.soup.title.string if context.soup.title else None,
        }
        # Store the extracted data
        await context.push_data(data)
        # Extract links from the current page and add them to the crawling queue
        await context.enqueue_links()

    # Add the initial URL to the queue and start the crawl
    await crawler.run(['https://example.com'])

if __name__ == '__main__':
    asyncio.run(main())
</TabItem>
<TabItem value="PlaywrightCrawler" label="PlaywrightCrawler">
python
Copy code
import asyncio
from crawlee.playwright_crawler import PlaywrightCrawler, PlaywrightCrawlingContext

async def main() -> None:
    # Create an instance of PlaywrightCrawler
    crawler = PlaywrightCrawler()

    # Define a request handler to process each crawled page
    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -> None:
        # Extract relevant data from the page context
        data = {
            'url': context.request.url,
            'title': await context.page.title(),
        }
        # Store the extracted data
        await context.push_data(data)
        # Extract links from the current page and add them to the crawling queue
        await context.enqueue_links()

    # Add the initial URL to the queue and start the crawl
    await crawler.run(['https://example.com'])

if __name__ == '__main__':
    asyncio.run(main())
</TabItem>
</Tabs>
When you run the example, Crawlee will automate the data extraction process according to the selected crawler type.

Customizing Crawling Behavior
You can customize Crawlee's behavior by adjusting parameters such as max_requests_per_crawl and handling various exceptions that may occur during crawling.

python
Copy code
# Example of customizing the PlaywrightCrawler
async def main() -> None:
    crawler = PlaywrightCrawler(
        max_requests_per_crawl=50,
        headless=False,  # Run with a visible browser window
        browser_type='firefox'  # Use Firefox browser
    )

    # Add more customization as needed
    # ...

    await crawler.run(['https://example.com'])
Viewing Results
By default, Crawlee stores data in the ./storage directory within your current working directory. Results of the crawl will be saved as JSON files under ./storage/datasets/default/.

To view the results, you can use commands like cat or open the JSON files directly:

sh
Copy code
cat ./storage/datasets/default/000000001.json
Further Reading
For more examples and in-depth documentation on Crawlee, visit the Crawlee Documentation and explore the Examples section.
```
