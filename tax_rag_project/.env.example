# Environment Configuration Template
# Copy this file to .env and customize for your environment

# ============================================================================
# CRAWLER SETTINGS
# ============================================================================

# Maximum number of pages to crawl per run
# Set to 0 for unlimited (use with caution)
MAX_REQUESTS_PER_CRAWL=1000

# Number of concurrent requests
# Higher values = faster crawling, but more resource intensive
MAX_CONCURRENCY=5

# Request timeout in seconds
REQUEST_TIMEOUT=30

# User-Agent string for crawler requests
# Identify your bot clearly and provide contact info
USER_AGENT="CRA-Tax-Documentation-Bot/1.0 (+https://yourdomain.com/bot-info)"

# ============================================================================
# RATE LIMITING
# ============================================================================

# Maximum requests per minute (hard limit to avoid overwhelming servers)
# Recommended: 60 for production, 10-20 for testing
MAX_REQUESTS_PER_MINUTE=60

# Minimum delay between requests in milliseconds
# Adds jitter to appear more human-like
MIN_DELAY_BETWEEN_REQUESTS_MS=100

# Maximum delay between requests in milliseconds
# Crawlee randomizes delays within this range
MAX_DELAY_BETWEEN_REQUESTS_MS=500

# Minimum delay between requests to the same domain (seconds)
# Respect server resources - recommended minimum: 1 second
MIN_REQUEST_DELAY=1.0

# Maximum delay between requests (for random jitter)
MAX_REQUEST_DELAY=3.0

# Maximum retries for failed requests
MAX_RETRIES=3

# Retry backoff multiplier (exponential backoff)
RETRY_BACKOFF_MULTIPLIER=2.0

# ============================================================================
# SECURITY & POLITENESS
# ============================================================================

# Respect robots.txt directives (recommended: true)
# Set to false only if you have explicit permission to bypass robots.txt
RESPECT_ROBOTS_TXT=true

# ============================================================================
# STORAGE SETTINGS
# ============================================================================

# Base directory for local storage
# Crawlee will create subdirectories for datasets, key-value stores, etc.
STORAGE_DIR=./storage

# Dataset name for crawled documents
DATASET_NAME=rag_tax_documents

# Key-value store name for metadata
KV_STORE_NAME=crawler_metadata

# ============================================================================
# QDRANT CLOUD SETTINGS
# ============================================================================

# Get your credentials at: https://cloud.qdrant.io
# 1. Create a free account (1GB storage included)
# 2. Create a new cluster
# 3. Copy your cluster URL and API key below

# Qdrant Cloud URL (required)
# Format: https://your-cluster-id.cloud.qdrant.io
QDRANT_URL=https://your-cluster.cloud.qdrant.io

# Qdrant Cloud API key (required)
# Get from your cluster dashboard
QDRANT_API_KEY=your-api-key-here

# Qdrant collection name for storing document embeddings
QDRANT_COLLECTION=tax_documents

# Enable Qdrant vector database integration
# Set to true when ready to store embeddings
USE_QDRANT=true

# ============================================================================
# EMBEDDING SETTINGS
# ============================================================================

# OpenAI Embeddings Configuration
# Model: text-embedding-3-small (1536 dimensions, cost-effective)
EMBEDDING_MODEL=text-embedding-3-small

# Batch size for embedding generation
# Adjust based on API rate limits (10 is good for testing, 32 for production)
EMBEDDING_BATCH_SIZE=10

# OpenAI API Key
# Get your API key from: https://platform.openai.com/api-keys
# IMPORTANT: Keep this secret! Never commit to version control.
OPENAI_API_KEY=sk-proj-...

# ============================================================================
# DOCUMENT PROCESSING
# ============================================================================

# Chunk size for text splitting (in tokens/characters)
# Smaller chunks = more precise retrieval, larger chunks = more context
CHUNK_SIZE=800

# Overlap between chunks (in tokens/characters)
# Helps maintain context across chunk boundaries
CHUNK_OVERLAP=200

# Minimum chunk size (discard smaller chunks)
MIN_CHUNK_SIZE=100

# Maximum chunk size (hard limit)
MAX_CHUNK_SIZE=1500

# ============================================================================
# LOGGING
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format
# Options: json, text
LOG_FORMAT=text

# Log file path (leave empty to log to console only)
LOG_FILE=

# ============================================================================
# CRAWLER TARGETS
# ============================================================================

# Starting URLs for the crawler (comma-separated)
# CRA tax documentation pages
START_URLS=https://www.canada.ca/en/revenue-agency.html

# URL patterns to include (regex, comma-separated)
# Only crawl URLs matching these patterns
INCLUDE_URL_PATTERNS=/en/revenue-agency/.*,/en/services/tax/.*

# URL patterns to exclude (regex, comma-separated)
# Skip URLs matching these patterns (e.g., login pages, search results)
EXCLUDE_URL_PATTERNS=/login,/search,/auth

# Maximum crawl depth from start URLs
# 0 = only start URLs, 1 = start URLs + direct links, etc.
MAX_CRAWL_DEPTH=2

# Enable automatic link discovery and following
# Set to false to only crawl explicitly provided URLs
FOLLOW_LINKS=true

# ============================================================================
# PRODUCTION / DEPLOYMENT
# ============================================================================

# Environment: development, production
ENVIRONMENT=development

# Enable debug mode (verbose logging, no external requests in some cases)
DEBUG=false

# Monitoring/alerting webhook URL (optional)
ALERT_WEBHOOK_URL=

# Health check port (for monitoring)
HEALTH_CHECK_PORT=8080
